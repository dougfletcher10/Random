{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pyspark / sparksql\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, FloatType\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# models\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, FeatureHasher, Normalizer\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, TrainValidationSplit, ParamGridBuilder, _parallelFitTasks, TrainValidationSplitModel\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "team_folder = \"dbfs:/mnt/mids-w261/team17\"\n",
    "dbutils.fs.ls(team_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"generic_name\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding columns - all sql functions available in \"f\" library of spark including lag / lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_tail_num_flight_delay(airlines_with_tz):\n",
    "  # window spec to get the last flight for the tail number\n",
    "  tailnum_window_spec = \\\n",
    "    Window \\\n",
    "      .partitionBy([f.col(\"tail_num\")]) \\\n",
    "      .orderBy(f.col(\"scheduled_flight_datetime_UTC\")) \\\n",
    "      .rowsBetween(-1, -1)\n",
    "\n",
    "  # create columns for:\n",
    "  # 1. tail_num_delay_lag_tem = the delay value for the last flight that had the same tail number\n",
    "  # 2. tail_num_dep_time_lag = when was the last flight departure that had the same tail number, in UTC\n",
    "  # 3. tail_num_from = where did the last flight from this tail number come from\n",
    "  # 4. tail_num_to = what was destination of the last flight from this tail_num\n",
    "  # 5. tail_num_expected_elapsed = what is the scheduled elapsed time for the previous flight\n",
    "  airlines_with_tz = airlines_with_tz \\\n",
    "    .withColumn(\"tail_num_delay_lag_temp\", f.lag(\"dep_delay\", 1).over(tailnum_window_spec)) \\\n",
    "    .withColumn(\"tail_num_dep_time_lag\", f.lag(\"scheduled_flight_datetime_UTC\", 1).over(tailnum_window_spec)) \\\n",
    "    .withColumn(\"tail_num_from\", f.lag(\"origin\", 1).over(tailnum_window_spec)) \\\n",
    "    .withColumn(\"tail_num_to\", f.lag(\"dest\", 1).over(tailnum_window_spec)) \\\n",
    "    .withColumn(\"tail_num_expected_elapsed\", f.lag(\"crs_elapsed_time\", 1).over(tailnum_window_spec))\n",
    "\n",
    "  # if the last departure was between 2 and 8 hrs ago, record the previous delay, else 0\n",
    "  airlines_with_tz = airlines_with_tz.withColumn(\"tail_num_delay_lag\",\n",
    "    f.when((f.col(\"scheduled_flight_datetime_UTC\").cast(LongType()) - \n",
    "            f.col(\"tail_num_dep_time_lag\").cast(LongType())).between(2*3600, 8*3600), \n",
    "           f.col(\"tail_num_delay_lag_temp\")) \\\n",
    "    .otherwise(0))\n",
    "  \n",
    "  # if the new expected arrival time with the observed delay is within two hours of the current flight departure time, indicate number of min else 0\n",
    "  # last destination must also match current origin\n",
    "  airlines_with_tz = airlines_with_tz.withColumn(\"tail_num_arrival_to_next_departure_delta\",\n",
    "      (f.col(\"scheduled_flight_datetime_UTC\").cast(LongType()) - \n",
    "      (f.col(\"tail_num_dep_time_lag\").cast(LongType()) + f.col(\"tail_num_expected_elapsed\")*60 + f.col(\"tail_num_delay_lag\")*60)) / 60)\n",
    "  \n",
    "  airlines_with_tz = airlines_with_tz.withColumn(\"tail_num_arrival_to_next_departure_delta\",\n",
    "    f.when((f.col(\"tail_num_arrival_to_next_departure_delta\") > 120) &\n",
    "           (f.col(\"tail_num_to\") == f.col(\"origin\")), 120) \\\n",
    "    .when(f.col(\"tail_num_arrival_to_next_departure_delta\").isNull(), 120) \\\n",
    "    .otherwise(f.col(\"tail_num_arrival_to_next_departure_delta\")))\n",
    "  \n",
    "  # tail num arriving less than two hours before next departure indicator\n",
    "  airlines_with_tz = airlines_with_tz.withColumn(\"tail_num_arrive_less_than_two_hours_indicator\",\n",
    "    f.when(f.col(\"tail_num_arrival_to_next_departure_delta\") < 120, 1).otherwise(0))\n",
    "  \n",
    "  airlines_with_tz = airlines_with_tz.drop(\"tail_num_delay_lag_temp\", \"tail_num_dep_time_lag\", \"tail_num_to\", \n",
    "                                           \"tail_num_expected_elapsed\", \"tail_num_delay_lag\")\n",
    "  return airlines_with_tz.fillna({'tail_num_from':'NA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add complex struct types schema udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "# parse wind fields\n",
    "wind_schema = StructType([\n",
    "    StructField(\"direction\", FloatType(), False),\n",
    "    StructField(\"dir_obs_quality\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), False),\n",
    "    StructField(\"speed\", FloatType(), False),\n",
    "    StructField(\"speed_obs_quality\", StringType(), False)\n",
    "])\n",
    "\n",
    "def wind(s):\n",
    "    wind = s.split(',') if s else [float('nan'), None, None, float('nan'), None]\n",
    "    wind[0], wind[3] = float(wind[0]), float(wind[3])\n",
    "    return wind\n",
    "  \n",
    "# parse sky ceiling\n",
    "# cavok = ceiling and visibility okay\n",
    "ceiling_schema = StructType([\n",
    "    StructField(\"height\", FloatType(), False),\n",
    "    StructField(\"obs_quality\", StringType(), False),\n",
    "    StructField(\"determination\", StringType(), False),\n",
    "    StructField(\"cavok\", StringType(), False)\n",
    "]) \n",
    "\n",
    "def sky_ceiling(s):\n",
    "    sky = s.split(',') if s else [float('nan'), None, None, None]\n",
    "    sky[0] = float(sky[0])\n",
    "    return sky\n",
    "  \n",
    "# parse visibility\n",
    "visibility_schema = StructType([\n",
    "    StructField(\"visibility\", FloatType(), False),\n",
    "    StructField(\"vis_obs_quality\", StringType(), False),\n",
    "    StructField(\"variability\", StringType(), False),\n",
    "    StructField(\"var_obs_quality\", StringType(), False)\n",
    "]) \n",
    "\n",
    "def visibility(s):\n",
    "    vis = s.split(',') if s else [float('nan'), None, None, None]\n",
    "    vis[0] = float(vis[0])\n",
    "    return vis\n",
    "\n",
    "# just return the first measurement value as a float for [temperature, dew temp, pressure]\n",
    "def first_value(s, sep=',', t=float):\n",
    "    return t(s.split(sep)[0])\n",
    "\n",
    "# precipitation\n",
    "precipitation_schema = StructType([\n",
    "    StructField(\"period\", FloatType(), False),\n",
    "    StructField(\"depth\", FloatType(), False),\n",
    "    StructField(\"condition\", StringType(), False),\n",
    "    StructField(\"precip_quality\", StringType(), False)\n",
    "]) \n",
    "def precipitation(s):\n",
    "    precip = s.split(',') if s else [-1, -1, '-1', '-1']\n",
    "    precip[0], precip[1] = float(precip[0]), float(precip[1])\n",
    "    return precip\n",
    "  \n",
    "# snow\n",
    "snow_schema = StructType([\n",
    "    StructField(\"depth\", FloatType(), False),\n",
    "    StructField(\"condition\", StringType(), False),\n",
    "    StructField(\"snow_quality\", StringType(), False),\n",
    "    StructField(\"water_depth\", StringType(), False)\n",
    "]) \n",
    "def snow(s):\n",
    "    snow = s.split(',')[:4] if s else [-1, '-1', '-1', -1]\n",
    "    snow[0], snow[3] = float(snow[0]), float(snow[3])\n",
    "    return snow\n",
    "\n",
    "wind_udf = udf(wind, wind_schema)\n",
    "sky_udf = udf(sky_ceiling, ceiling_schema)\n",
    "vis_udf = udf(visibility, visibility_schema)\n",
    "precip_udf = udf(precipitation, precipitation_schema)\n",
    "snow_udf = udf(snow, snow_schema)\n",
    "first_value_udf = udf(first_value, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manual sql in spark instance --> must create temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_features_full.createOrReplaceTempView(\"airlineFeaturesFullTeam17\")\n",
    "weather_features_full.createOrReplaceTempView(\"weatherFeaturesFullTeam17\")\n",
    "weather_feature_names = [f\"o.{c} as origin_{c}\" for c in weather_features_full.columns]\n",
    "weather_feature_names.extend([f\"d.{c} as dest_{c}\" for c in weather_features_full.columns])\n",
    "\n",
    "joined_full = spark.sql(\"\"\"\n",
    "  SELECT a.*, {weather_columns}\n",
    "  FROM airlineFeaturesFullTeam17 a\n",
    "  JOIN weatherFeaturesFullTeam17 o\n",
    "    ON ('K'||a.origin) = o.call_sign\n",
    "    AND a.scheduled_flight_hour_UTC = o.join_datetime\n",
    "  JOIN weatherFeaturesFullTeam17 d\n",
    "    ON ('K'||a.dest) = d.call_sign\n",
    "    AND a.scheduled_flight_hour_UTC = d.join_datetime;\n",
    "\"\"\".format(weather_columns=\", \".join(weather_feature_names)))\n",
    "\n",
    "display(joined_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_full \\\n",
    ".write \\\n",
    ".mode('Overwrite') \\\n",
    ".parquet(team_folder + \"/joinedFeaturesFull.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kmeans clustering ++ union by name function --> must have same column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "joined_features_full = spark.read.option(\"header\", \"true\").parquet(team_folder + \"/joinedFeaturesFull.parquet\")\n",
    "\n",
    "origin_lat_long = joined_features_full.select(f.col(\"origin\").alias(\"airport_code\"), f.col(\"origin_latitude\").alias(\"latitude\"), f.col(\"origin_longitude\").alias(\"longitude\")).distinct()\n",
    "dest_lat_long = joined_features_full.select(f.col(\"dest\").alias(\"airport_code\"), f.col(\"dest_latitude\").alias(\"latitude\"), f.col(\"dest_longitude\").alias(\"longitude\")).distinct()\n",
    "\n",
    "airport_lat_long = origin_lat_long.unionByName(dest_lat_long).toPandas()\n",
    "\n",
    "airport_lat_long['kmeans_cluster_label'] = KMeans(n_clusters=6, random_state=0).fit_predict(airport_lat_long[['latitude', 'longitude']])\n",
    "\n",
    "sns.scatterplot(data=airport_lat_long, x='longitude', y='latitude', hue='kmeans_cluster_label', palette=\"deep\")\n",
    "plt.title(\"Airports Clustered by Lat, Long\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# udf and add / drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_by_date_and_region(data, datecolumn='FL_DATE', region=\"origin_region\", splits=[0.8,0.1,0.1], drop_date=True):\n",
    "  def hashAndAssign(x, splits):\n",
    "    hash_val = hash(x)\n",
    "    perc = (hash_val % 1000) / 1000.0\n",
    "    for i, split in enumerate(splits):\n",
    "      if perc < split:\n",
    "        return i\n",
    "      perc -= split\n",
    "    return i\n",
    "  hash_and_assign_udf = f.udf(lambda x: hashAndAssign(x, splits), IntegerType())\n",
    "  \n",
    "  data = data.withColumn('data_group', hash_and_assign_udf(f.concat_ws('_',f.col(datecolumn),f.col(region))))\n",
    "  if drop_date:\n",
    "    data = data.drop(datecolumn)\n",
    "  return data\n",
    "\n",
    "\n",
    "def split_data_by_column(data, col_name=\"data_group\"):\n",
    "  train = data.filter(f.col(col_name) == 0).drop(col_name)\n",
    "  valid = data.filter(f.col(col_name) == 1).drop(col_name)\n",
    "  test = data.filter(f.col(col_name) == 2).drop(col_name)\n",
    "  return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop na / onehot assembler / vector assembler / pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorical_feature_transformations_linear(onehot_cols, hash_cols):\n",
    "  # hash encode categorical features with high cardinality\n",
    "  hasher = FeatureHasher(num_features=400, inputCols=hash_cols, outputCol=\"hashFeatures\")\n",
    "  \n",
    "  # one hot encode features with low cardinality\n",
    "  onehotIndexers = []\n",
    "  onehotInputs = []\n",
    "  onehotFeatures = []\n",
    "  for c in onehot_cols:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\")\n",
    "    onehotIndexers.append(indexer)\n",
    "    onehotInputs.append(indexer.getOutputCol())\n",
    "    onehotFeatures.append(f\"{c}_onehot\")\n",
    "  one_hot = OneHotEncoder(inputCols=onehotInputs, outputCols=onehotFeatures)\n",
    "  \n",
    "  # return output columns and transformation objects\n",
    "  return [\"hashFeatures\"] + onehotFeatures, onehotIndexers + [one_hot, hasher]\n",
    "\n",
    "def categorical_feature_transformations_trees(cols):\n",
    "  indexers = []\n",
    "  features = []\n",
    "  # simply index all categories\n",
    "  for c in cols:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\")\n",
    "    indexers.append(indexer)\n",
    "    features.append(f\"{c}_indexed\")\n",
    "  \n",
    "  # return output columns and transformation objects\n",
    "  return features, indexers\n",
    "\n",
    "def feature_transformation_pipeline(df, numerical_cols, onehot_cols, hash_cols, output, transform_categorical=True):\n",
    "  pipeline_stages = []\n",
    "  # normalize numerical columns\n",
    "  num_assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numericalFeatures\")\n",
    "  normalizer = Normalizer(inputCol=\"numericalFeatures\", outputCol=\"normFeatures\")\n",
    "  pipeline_stages.extend([num_assembler, normalizer])\n",
    "  \n",
    "  if transform_categorical: \n",
    "    # transform_categorical should be true if model requires vectorization of categorical feature, e.g. Linear Regression\n",
    "    catFeatureCols, catStages = categorical_feature_transformations_linear(onehot_cols, hash_cols)\n",
    "  else:\n",
    "    # if tree based model, don't need to transform categorical features, just index\n",
    "    catFeatureCols, catStages = categorical_feature_transformations_trees(onehot_cols + hash_cols)\n",
    "    \n",
    "  pipeline_stages.extend(catStages)\n",
    "  \n",
    "  # combine all feature transformations into a single feature vector\n",
    "  assembler = VectorAssembler(inputCols=[\"normFeatures\"] + catFeatureCols, outputCol=\"features\")\n",
    "  pipeline_stages.append(assembler)\n",
    "  pipeline = Pipeline(stages=pipeline_stages)\n",
    "  return pipeline\n",
    "\n",
    "def full_model_pipeline():\n",
    "  raise NotImplementedError\n",
    "\n",
    "\n",
    "#function to convert df into consumable format for MLlib\n",
    "def data_converter(df, numerical_cols, onehot_cols, hash_cols, output, transform_categorical=True, splits=[0.8,0.1,0.1], seed=0):\n",
    "  pipe = feature_transformation_pipeline(df, numerical_cols, onehot_cols, hash_cols, output, transform_categorical=transform_categorical)\n",
    "  \n",
    "  tmp = df[numerical_cols + onehot_cols + hash_cols + [output, \"FL_DATE\"]]\n",
    "  tmp = tmp.dropna()\n",
    "  for c in onehot_cols + hash_cols:\n",
    "    tmp = tmp.withColumn(c, f.col(c).cast(StringType()))\n",
    "  data = split_by_date_and_region(tmp, \n",
    "                                  datecolumn=\"FL_DATE\", \n",
    "                                  region=\"origin_region\" if \"origin_region\" in tmp.columns else \"origin\",\n",
    "                                  splits=splits)\n",
    "  return data, pipe\n",
    "\n",
    "\n",
    "def balance_classes(dataset, output_col, ratio=3.0, seed=2020):\n",
    "  ones = dataset.where(f.col(output_col)==1).sample(True, ratio, seed=seed)\n",
    "  zeroes = dataset.where(f.col(output_col)==0)\n",
    "  train_final = zeroes.union(ones).orderBy(f.rand())\n",
    "  return train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hasattr method confusion matrix, precision / recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(logit):\n",
    "  return 1 / (1 + math.exp(-logit))\n",
    "sigmoid_udf = f.udf(sigmoid, FloatType())\n",
    "      \n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "extract_prob_udf = udf(lambda v: ith_(v, 1), DoubleType())\n",
    "\n",
    "def format_predictions(predictions_df, output_col='DEP_DEL15'):\n",
    "  #if regression based model, transform prediction with sigmoid and create label\n",
    "  if not hasattr(predictions_df, 'probability'): \n",
    "    print(\"regression\")\n",
    "    predictions_df = predictions_df \\\n",
    "      .withColumn(\"label\", (f.col(output_col) >= 15).cast(FloatType())) \\\n",
    "      .withColumn(\"delay_prob\", sigmoid_udf(f.col(\"prediction\") - 14.5))\n",
    "  else:\n",
    "    print(\"classification\")\n",
    "    predictions_df = predictions_df \\\n",
    "      .withColumn(\"label\", (f.col(output_col)).cast(FloatType())) \\\n",
    "      .withColumn(\"delay_prob\", extract_prob_udf(\"probability\"))\n",
    "  return predictions_df\n",
    "\n",
    "from collections import namedtuple\n",
    "ConfusionMatrix = namedtuple('ConfusionMatrix', ['TP', 'FP', 'TN', 'FN'])\n",
    "\n",
    "def accuracy(confusion):\n",
    "  return (confusion.TP + confusion.TN) / sum(confusion)\n",
    "\n",
    "def accuracy_of_common_class_prediction(confusion):\n",
    "  pos_class_prediction_accuracy = (confusion.TP + confusion.FN) / sum(confusion)\n",
    "  return max(pos_class_prediction_accuracy, 1-pos_class_prediction_accuracy)\n",
    "\n",
    "def precision(confusion):\n",
    "  try:\n",
    "    return (confusion.TP) / (confusion.TP + confusion.FP)\n",
    "  except ZeroDivisionError:\n",
    "    return float('nan')\n",
    "\n",
    "def recall(confusion):\n",
    "  return (confusion.TP) / (confusion.TP + confusion.FN)\n",
    "\n",
    "def f1_score(confusion):\n",
    "  prec = precision(confusion)\n",
    "  rec = recall(confusion)\n",
    "  return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def categorize_prediction(pred, true):\n",
    "  # [TP, FP, TN, FN]\n",
    "  confusion = [0, 0, 0, 0]\n",
    "  if pred == 1 and true == 1:\n",
    "    confusion[0] = 1\n",
    "  elif pred == 1 and true == 0:\n",
    "    confusion[1] = 1\n",
    "  elif pred == 0 and true == 0:\n",
    "    confusion[2] = 1\n",
    "  else:\n",
    "    confusion[3] = 1\n",
    "  return confusion\n",
    "\n",
    "confusion_udf = f.udf(categorize_prediction, IntegerType())\n",
    "\n",
    "def calc_metrics_at_thresh(df, prediction='probability', outcome='label', thresh=0.5, verbose=True):\n",
    "  df = df.withColumn('prediction_at_thresh', (f.col(prediction) >= thresh).cast(IntegerType()))\n",
    "  \n",
    "  confusion = df.select([\"prediction_at_thresh\", outcome]).rdd \\\n",
    "    .map(lambda pred_truth: categorize_prediction(pred_truth[0], pred_truth[1])) \\\n",
    "    .reduce(lambda x, y: [sum(z) for z in zip(x, y)])\n",
    "  confusion_matrix = ConfusionMatrix(*confusion)\n",
    "  print(confusion_matrix)\n",
    "  \n",
    "  acc = accuracy(confusion_matrix)\n",
    "  prec = precision(confusion_matrix)\n",
    "  rec = recall(confusion_matrix)\n",
    "  f1 = f1_score(confusion_matrix)\n",
    "  \n",
    "  if verbose:\n",
    "    print(\"predicting frequent class would yield {:.4f} accuaracy\".format(accuracy_of_common_class_prediction(confusion_matrix)))\n",
    "    print(confusion_matrix)\n",
    "    print(\"accuracy: {:.4f}\".format(acc))\n",
    "    print(\"precision: {:.4f}\".format(prec))\n",
    "    print(\"recall: {:.4f}\".format(rec))\n",
    "    print(\"f1: {:.4f}\".format(f1))\n",
    "  metrics = {'confusion_matrix': confusion_matrix, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "  return metrics\n",
    "\n",
    "def confusion_by_thresh(df, prediction='probability', outcome='label'):\n",
    "  _min, _max = df.select(f.min(f.col(prediction), f.max(f.col(prediction)))).collect()[0]\n",
    "  \n",
    "  confusion_matrices = df.select(prediction, outcome).rdd \\\n",
    "    .flatMap(lambda x: [(thresh, categorize_prediction(x[0]>=thresh, x[1])) for thresh in np.arange(_min, _max, 0.01)]) \\\n",
    "    .reduceByKey(lambda x, y: [sum(z) for z in zip(x, y)]) \\\n",
    "    .mapValues(lambda x: ConfusionMatrix(*x)) \\\n",
    "    .collect()\n",
    "  \n",
    "  confusion_df = pd.DataFrame(confusion_matrices, columns=[\"threshold\", \"confusion\"])\n",
    "  confusion_df['accuracy'] = confusion_df[\"confusion\"].apply(lambda x: accuracy(x))\n",
    "  confusion_df['precision'] = confusion_df[\"confusion\"].apply(lambda x: precision(x))\n",
    "  confusion_df['recall'] = confusion_df[\"confusion\"].apply(lambda x: recall(x))\n",
    "  confusion_df['f1'] = confusion_df[\"confusion\"].apply(lambda x: f1_score(x))\n",
    "  return confusion_df\n",
    "\n",
    "def plot_pr_curve(confusion_by_thresh):\n",
    "  prec_rec = confusion_by_thresh[['precision', 'recall']].dropna().sort_values('recall')\n",
    "  sns.lineplot(data=prec_rec, x='recall', y='precision')\n",
    "  plt.title('Precision-Recall')\n",
    "  plt.show()\n",
    "\n",
    "def plot_predictions_by_label(predictions, prediction='probability', outcome='label', fractions={0: 0.005, 1: 0.001}):\n",
    "  sampled = predictions.sampleBy(outcome, fractions=fractions, seed=0).select(probability, outcome).toPandas()\n",
    "  sns.distplot(sampled[prediction][sampled[outcome]==0], label=\"Label=0\")\n",
    "  sns.distplot(sampled[prediction][sampled[outcome]==1], label=\"Label=1\")\n",
    "  plt.xlabel(\"predictions\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate classification metrics\n",
    "metrics = BinaryClassificationMetrics(predictions.select([\"delay_prob\", \"label\"]).rdd)\n",
    "print(\"PR AUC: {}\".format(metrics.areaUnderPR))\n",
    "print(\"ROC AUC: {}\".format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs_rf_model = tvs_rf.fit(data.withColumnRenamed('DEP_DEL15', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create massive sql query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weather_features(weather_parsed, where_clause=\"\"):\n",
    "  weather_parsed.createOrReplaceTempView(\"weather_parsed\")\n",
    "  weatherFeatures = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      date_trunc('HOUR', DATE) + INTERVAL 2 hours as join_datetime,\n",
    "      date,\n",
    "      trim(call_sign) as call_sign,\n",
    "\n",
    "      -- wind\n",
    "      wind_parsed.speed as wind_speed, \n",
    "      wind_parsed.speed - lag(wind_parsed.speed, 1) over (partition by call_sign order by date) as wind_speed_minus_1,\n",
    "      wind_parsed.speed - lag(wind_parsed.speed, 2) over (partition by call_sign order by date) as wind_speed_minus_2,\n",
    "      wind_parsed.speed - max(wind_parsed.speed) over (partition by call_sign order by date rows 2 preceding) as diff_to_max_wind,\n",
    "\n",
    "      -- visibility\n",
    "      visibility_parsed.visibility as visibility, \n",
    "      visibility_parsed.visibility - lag(visibility_parsed.visibility, 1) over (partition by call_sign order by date) as visibility_minus_1,\n",
    "      visibility_parsed.visibility - lag(visibility_parsed.visibility, 2) over (partition by call_sign order by date) as visibility_minus_2,\n",
    "      visibility_parsed.variability as visibility_variability,\n",
    "\n",
    "      -- sky_ceiling\n",
    "      sky_ceiling_parsed.height as ceiling, \n",
    "      sky_ceiling_parsed.height - lag(sky_ceiling_parsed.height, 1) over (partition by call_sign order by date) as ceiling_minus_1,\n",
    "      sky_ceiling_parsed.height - lag(sky_ceiling_parsed.height, 2) over (partition by call_sign order by date) as ceiling_minus_2,\n",
    "      sky_ceiling_parsed.cavok,\n",
    "\n",
    "      -- snow\n",
    "      snow_parsed.condition as snow_condition,\n",
    "      snow_parsed.depth as snow_depth, \n",
    "      snow_parsed.depth - lag(snow_parsed.depth, 1) over (partition by call_sign order by date) as snow_depth_minus_1,\n",
    "      snow_parsed.depth - lag(snow_parsed.depth, 2) over (partition by call_sign order by date) as snow_depth_minus_2,\n",
    "\n",
    "      -- precipitation\n",
    "      aa1_parsed.period as precip_period,\n",
    "      aa1_parsed.depth as precip_depth,\n",
    "      case when aa4_parsed.depth > 0 then aa4_parsed.depth\n",
    "           when aa3_parsed.depth > 0 then aa3_parsed.depth\n",
    "           when aa2_parsed.depth > 0 then aa2_parsed.depth\n",
    "           else aa1_parsed.depth end as max_precip_depth,\n",
    "      case when aa4_parsed.depth > 0 then aa4_parsed.period\n",
    "           when aa3_parsed.depth > 0 then aa3_parsed.period\n",
    "           when aa2_parsed.depth > 0 then aa2_parsed.period\n",
    "           else aa1_parsed.period end as max_precip_period,\n",
    "\n",
    "      -- pressure\n",
    "      pressure,\n",
    "      pressure - lag(pressure, 1) over (partition by call_sign order by date) as pressure_minus_1,\n",
    "      pressure - lag(pressure, 2) over (partition by call_sign order by date) as pressure_minus_2,\n",
    "\n",
    "      temperature,\n",
    "      dew_temperature,\n",
    "      latitude, \n",
    "      longitude, \n",
    "      elevation\n",
    "\n",
    "    FROM weather_parsed\n",
    "    {where_clause}\n",
    "    ORDER BY call_sign, date;\n",
    "    \"\"\".format(where_clause=where_clause))\n",
    "  return weatherFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
