{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Processing in Fannie Mae Sandbox\n",
    "### PySpark example\n",
    "\n",
    "This notebook shows a working example of using Amazon SageMaker Processing feature to run data preprocessing using PySpark workloads on the Amazon SageMaker platform within Fannie Mae Sandbox environment.  \n",
    "\n",
    "The data and the scripts used in this notebook are originally from the SageMaker example notebook: \\\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_processing/feature_transformation_with_sagemaker_processing/feature_transformation_with_sagemaker_processing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Prerequisite\n",
    "\n",
    "Taking the following steps to make sure that SageMaker will be upgraded to the latest version:\n",
    "\n",
    "- pip install sagemaker --upgrade\n",
    "- pip install pyarrow\n",
    "- check the version of SageMaker installed\n",
    "- ensure that sagemaker_notebook_config.py is also uploaded to the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the SageMaker version shown above >= 2.11, it is good to move forward.  If not, please check if the notebook kernel is restarted after pip install and rerun the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Configuration\n",
    "\n",
    "\n",
    "- setup existing bucket name as the default bucket of SageMaker Session to bypass `CreateBucket` operation\n",
    "- retrieve notebook instance configuration\n",
    "- upload input data to S3, including both csv and parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "#from sagemaker_notebook_config import SageMakerNotebookConfig\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.network import NetworkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"fnma-dsmlp-devl-edl-us-east-1-edl\"\n",
    "user_home_folder = \"home/user/gaubxs\"  # the folder that was created in s3 bucket\n",
    "\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "# run the following line to bypass `try: create_bucket` block in sagemaker session:\n",
    "sagemaker_session._default_bucket = bucket\n",
    "\n",
    "# test if the default bucket has been set \n",
    "print(sagemaker_session.default_bucket())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageMakerNotebookConfig:\n",
    "    def __init__(self, notebook_instance_name=None, fp_resource_metadata=None):\n",
    "        self.fp_resource_metadata = fp_resource_metadata or self._fp_resource_metadata_default\n",
    "        self.notebook_metadata = self.get_resource_metadata()\n",
    "        self.notebook_instance_name = self.get_instance_name(notebook_instance_name)\n",
    "\n",
    "        self._sgm_client = boto3.client(\"sagemaker\")\n",
    "        self._ec2_client = boto3.client(\"ec2\")\n",
    "\n",
    "    @property\n",
    "    def _fp_resource_metadata_default(self):\n",
    "        return \"/opt/ml/metadata/resource-metadata.json\"\n",
    "\n",
    "    def get_resource_metadata(self):\n",
    "        with open(self.fp_resource_metadata) as fp:\n",
    "            metadata = json.load(fp)\n",
    "        return metadata\n",
    "\n",
    "    def get_instance_name(self, notebook_instance_name):\n",
    "        if notebook_instance_name and self.notebook_metadata[\"ResourceName\"] != notebook_instance_name:\n",
    "            message = \"Inconsistent Notebook Instance Name(s): \\n\"\n",
    "            message += f\"Input instance name: {notebook_instance_name} \\n\"\n",
    "            message += \"Instance name found in default metadata: {} \\n\".format(self.notebook_metadata[\"ResourceName\"])\n",
    "            message += \"Will use the input instance name. \\n\"\n",
    "            warnings.warn(message, ResourceWarning)\n",
    "\n",
    "            return notebook_instance_name\n",
    "        else:\n",
    "            return self.notebook_metadata[\"ResourceName\"]\n",
    "    \n",
    "    @property\n",
    "    def desc_nb_instance(self):\n",
    "        return self._sgm_client.describe_notebook_instance(NotebookInstanceName=self.notebook_instance_name)\n",
    "    \n",
    "    @property\n",
    "    def notebook_instance_arn(self):\n",
    "        return self.desc_nb_instance[\"NotebookInstanceArn\"]\n",
    "    \n",
    "    @property\n",
    "    def account_id(self):\n",
    "        return self.notebook_instance_arn.split(\":\")[4]\n",
    "    \n",
    "    @property\n",
    "    def instance_region(self):\n",
    "        return self.notebook_instance_arn.split(\":\")[3]\n",
    "    \n",
    "    @property\n",
    "    def subnet_id(self):\n",
    "        return self.desc_nb_instance.get(\"SubnetId\", None)\n",
    "    \n",
    "    @property\n",
    "    def security_groups(self):\n",
    "        return self.desc_nb_instance.get(\"SecurityGroups\", [])\n",
    "\n",
    "    @property\n",
    "    def role(self):\n",
    "        return self.desc_nb_instance.get(\"RoleArn\", \"\")\n",
    "    \n",
    "    @property\n",
    "    def kms_id(self):\n",
    "        return self.desc_nb_instance.get(\"KmsKeyId\", \"\")\n",
    "    \n",
    "    @property\n",
    "    def kms_key(self):\n",
    "        return self.kms_id.split(\"key/\")[1]\n",
    "    \n",
    "    @property\n",
    "    def tags(self):\n",
    "        tags = []\n",
    "        for tag_dict in self._sgm_client.list_tags(ResourceArn=self.notebook_instance_arn).get(\"Tags\", []):\n",
    "            if not tag_dict[\"Key\"].startswith(\"aws\"):\n",
    "                tags.append(tag_dict)\n",
    "        return tags\n",
    "    \n",
    "    def get_tag_value(self, key_name):\n",
    "        for tag in self.tags:\n",
    "            if tag[\"Key\"] == key_name:\n",
    "                return tag[\"Value\"]\n",
    "        \n",
    "        message = f\"Key name: {key_name} not found in tags \\n\"\n",
    "        message += \"Return None\"\n",
    "        warnings.warn(message)\n",
    "        return None\n",
    "\n",
    "    def vpc_config(self, tag_key_filter=[\"Function\"]):\n",
    "        # need to get the vpc_id first\n",
    "        # use the subnet_id of the current notebook instance\n",
    "        # to retrieve the vpc_id\n",
    "        \n",
    "        desc_current_subnet = self._ec2_client.describe_subnets(SubnetIds=[self.subnet_id])[\"Subnets\"][0]\n",
    "        \n",
    "        vpc_id = desc_current_subnet[\"VpcId\"]\n",
    "        \n",
    "        # create a tag filter based on the input tag_key_filter \n",
    "        # and the corresponding value of the current subnet\n",
    "        \n",
    "        tag_filter = []\n",
    "        for key in tag_key_filter:\n",
    "            for tag in desc_current_subnet[\"Tags\"]:\n",
    "                if tag[\"Key\"] == key:\n",
    "                    value = tag[\"Value\"]\n",
    "                    break\n",
    "            \n",
    "            tag_filter.append({\"Key\": key, \"Value\": value})\n",
    "        \n",
    "        # use filtering to get a list subnets inside the same vpc\n",
    "        \n",
    "        filters = [{\"Name\": \"vpc-id\", \"Values\": [vpc_id]}]\n",
    "        \n",
    "        for tag in tag_filter:\n",
    "            filters.append({\"Name\": \"tag:{}\".format(tag[\"Key\"]), \"Values\": [tag[\"Value\"]]})\n",
    "        \n",
    "        subnets = [subnet[\"SubnetId\"] for subnet in self._ec2_client.describe_subnets(Filters=filters)[\"Subnets\"]]\n",
    "        \n",
    "        return {\n",
    "            \"SecurityGroupIds\": self.security_groups,\n",
    "            \"Subnets\": subnets\n",
    "        }\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SageMakerNotebookConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Download the sample dataset and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
    "\n",
    "column_names = [\n",
    "    \"sex\", \n",
    "    \"length\", \n",
    "    \"diameter\", \n",
    "    \"height\", \n",
    "    \"whole_weight\", \n",
    "    \"shucked_weight\", \n",
    "    \"viscera_weight\", \n",
    "    \"shell_weight\", \n",
    "    \"rings\", \n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"abalone.csv\", header=None, names=column_names)\n",
    "df.to_parquet(\"abalone.parquet\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload csv data file to S3\n",
    "input_data = f\"s3://{bucket}/{user_home_folder}/sagemaker_processing/examples/abalone/input/abalone.csv\"\n",
    "!aws s3 cp abalone.csv $input_data --sse aws:kms --sse-kms-key-id $config.kms_id\n",
    "\n",
    "# upload parquet data file to S3\n",
    "input_data = f\"s3://{bucket}/{user_home_folder}/sagemaker_processing/examples/abalone/input/abalone.parquet\"\n",
    "!aws s3 cp abalone.parquet $input_data --sse aws:kms --sse-kms-key-id $config.kms_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preprocessing by using SageMaker PySparkProcessor\n",
    "\n",
    "1. create the pyspark processing script\n",
    "2. Upload the script to S3\n",
    "3. Upload the data needs to be processed to S3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***\n",
    "At high level DataProcessing is divided in to three parts\n",
    "1) Reading the data\n",
    "2) Processing the data\n",
    "3) Saving the output (processed data)\n",
    "\n",
    "Reading Data:  your code should refer the file path when reading the data as in this example the data will be uploaded to file system path.\n",
    "\n",
    "Saving the output:  the output will be saved to file system and then copied to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocess_pyspark.py\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "print(\"&\"*100)\n",
    "import glob\n",
    "root_dir = '/opt/ml/processing'\n",
    "input_data = 'file:///opt/ml/processing/input-data'\n",
    "output_data = 'file:///opt/ml/processing/output-data'\n",
    "\n",
    "for filename in glob.iglob(root_dir + '**/**', recursive=True):\n",
    "     print(filename)\n",
    "for currentpath, folders, files in os.walk(root_dir):\n",
    "    print(currentpath, folders, files)\n",
    "\n",
    "\n",
    "\n",
    "def csv_line(data):\n",
    "    r = ','.join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "def main():\n",
    "    print(\"start preprocessing job.\")\n",
    "    print(\"%\"*100)\n",
    "    spark = SparkSession.builder.appName(\"PySparkAbalone\").getOrCreate()\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"set up schema\")\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType([StructField(\"sex\", StringType(), True), \n",
    "                         StructField(\"length\", DoubleType(), True),\n",
    "                         StructField(\"diameter\", DoubleType(), True),\n",
    "                         StructField(\"height\", DoubleType(), True),\n",
    "                         StructField(\"whole_weight\", DoubleType(), True),\n",
    "                         StructField(\"shucked_weight\", DoubleType(), True),\n",
    "                         StructField(\"viscera_weight\", DoubleType(), True), \n",
    "                         StructField(\"shell_weight\", DoubleType(), True), \n",
    "                         StructField(\"rings\", DoubleType(), True)])\n",
    "\n",
    "    \n",
    "    print(\"read parquet\")\n",
    "    print(\"!\"*100)\n",
    "    \n",
    "    ###########################################################################\n",
    "    ###################### reading data #######################################\n",
    "    ###########################################################################\n",
    "    #total_df = spark.read.parquet('file:///opt/ml/processing/input/abalone.parquet')\n",
    "    total_df = spark.read.parquet(input_data)\n",
    "    print(total_df.head())\n",
    "    \n",
    "    print(\"Successfully read parquet file as input\")\n",
    "    \n",
    "    ###########################################################################\n",
    "    ###################### data processing ####################################\n",
    "    ###########################################################################\n",
    "\n",
    "    #StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "    \n",
    "    #one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    #vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(inputCols=[\"sex_vec\", \n",
    "                                           \"length\", \n",
    "                                           \"diameter\", \n",
    "                                           \"height\", \n",
    "                                           \"whole_weight\", \n",
    "                                           \"shucked_weight\", \n",
    "                                           \"viscera_weight\", \n",
    "                                           \"shell_weight\"], \n",
    "                                outputCol=\"features\")\n",
    "    \n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "    \n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "    \n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "    \n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    #train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    #train_lines = train_rdd.map(csv_line)\n",
    "    #train_lines.saveAsTextFile('s3a://' + os.path.join(args['s3_output_bucket'], args['s3_output_key_prefix'], 'train'))\n",
    "    \n",
    "\n",
    "    ###########################################################################\n",
    "    ###################### saving output ######################################\n",
    "    ###########################################################################\n",
    "    #transformed_file_name='transformed.parquet'\n",
    "    #validation_file_name='validate.parquet'\n",
    "    transformed_total_df.write.parquet(output_data+'/transformed')\n",
    "    validation_df.write.parquet(output_data+'/validate')\n",
    "    \n",
    "    for currentpath, folders, files in os.walk(output_data):\n",
    "        print(currentpath, folders, files)\n",
    "\n",
    "    \n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    #validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    #validation_lines = validation_rdd.map(csv_line)\n",
    "    #validation_lines.saveAsTextFile('s3a://' + os.path.join(args['s3_output_bucket'], args['s3_output_key_prefix'], 'validation'))\n",
    "    \n",
    "    # Here add one line to output the validation data to S3 in parquet format\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"#\"*100)\n",
    "    print(\"end of job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_code = f\"s3://{bucket}/{user_home_folder}/sagemaker_processing/examples/abalone/input/preprocess_pyspark.py\"\n",
    "!aws s3 cp preprocess_pyspark.py $input_code --sse aws:kms --sse-kms-key-id $config.kms_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data =  f\"s3://{bucket}/{user_home_folder}/sagemaker_processing/examples/abalone/input/abalone.parquet\"\n",
    "output_location = f\"s3://{bucket}/{user_home_folder}/sagemaker_processing/spark/census-income/output\"\n",
    "spark_log_location = f\"s3://{bucket}/{user_home_folder}/store-spark-events\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "`/opt/program/submit` is the entry point of the pre-built Spark docker container.  Inside the container, `/opt/program/submit` is a python script that execute the input python script as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. run sagemaker PySparkProcessor to process the data\n",
    "\n",
    "1. Create the PySparkProcessor object: for list of parameters refer: https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor\n",
    "2. Run the PySparkProcessor to perform the data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    \n",
    "    framework_version=\"2.4\",\n",
    "    role=config.role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    py_version=\"py37\",\n",
    "    container_version=\"1\",\n",
    "    volume_kms_key=config.kms_id, \n",
    "    output_kms_key=config.kms_id,\n",
    "    base_job_name='dsmlp-usr-gaubxs', \n",
    "    sagemaker_session=sagemaker_session, \n",
    "    tags=config.tags, \n",
    "    network_config=NetworkConfig(\n",
    "        enable_network_isolation=False, # the processing instance need to communicate with S3\n",
    "        encrypt_inter_container_traffic=True,\n",
    "        security_group_ids=config.vpc_config()['SecurityGroupIds'],\n",
    "        subnets=config.vpc_config()['Subnets'],\n",
    "    ),\n",
    "    \n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor.run(\n",
    "    submit_app=input_code,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_data,\n",
    "            destination='/opt/ml/processing/input-data'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination= output_location,\n",
    "            output_name='train_data',\n",
    "            source='/opt/ml/processing/output-data'\n",
    "        ),\n",
    "    \n",
    "    ],\n",
    "    spark_event_logs_s3_uri= spark_log_location,\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gaubxs37",
   "language": "python",
   "name": "conda_gaubxs37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
